{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd854f9",
   "metadata": {},
   "source": [
    "<center><h1>DSCI-552 Final Project</h1>\n",
    "<br>\n",
    "Name: Yu-Hsuan Lu\n",
    "<br>\n",
    "USC ID: 8153799015\n",
    "<br>\n",
    "Github username: yuhsuanlu</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a83957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import imghdr\n",
    "import os\n",
    "import splitfolders\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import random\n",
    "from skimage.color import rgb2gray\n",
    "# from torchvision.datasets import ImageFolder\n",
    "\n",
    "import keras\n",
    "import PIL.Image as Image\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "import graphviz\n",
    "import pydot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05bcae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a470c8",
   "metadata": {},
   "source": [
    "## Transfer Learning for Image Classification\n",
    "It is highly recommended that you complete this project using Keras2 and Python. \n",
    "#### (a) In this problem, we are trying to build a classifier that distinguishes images of 20 bird species. You are provided with text data in twenty folders. \n",
    "#### (b) Data Exploration and Pre-processing\n",
    "i. Images in each class are given in separate folders. The file Classes.xlsx provides the classes assigned to the bird species images in each folder. Therefore, you encode your classes using one-hot encoding and Classes.xlsx.\n",
    "ii. Randomly select ⌈0.7ni⌉ images from each folder as your training set, ⌈0.15ni⌉ as validation set, and the rest as your test set, where ni is the number of images in folder i and ⌈x⌉ is the ceiling of x.\n",
    "iii. In order for all the images to have the same size, zero-pad or resize the images in your dataset. This can be done using various tools, including OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4c83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../Data/images'\n",
    "\n",
    "for image_class in os.listdir(dir_path):\n",
    "    class_path = os.path.join(dir_path, image_class)\n",
    "    for img in os.listdir(class_path):\n",
    "        image_path = os.path.join(dir_path, image_class, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781f74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['005.Crested_Auklet', '015.Lazuli_Bunting', '156.White_eyed_Vireo', '081.Pied_Kingfisher', '135.Bank_Swallow', '086.Pacific_Loon', '067.Anna_Hummingbird', '076.Dark_eyed_Junco', '149.Brown_Thrasher', '127.Savannah_Sparrow', '041.Scissor_tailed_Flycatcher', '141.Artic_Tern', '082.Ringed_Kingfisher', '099.Ovenbird', '013.Bobolink', '104.American_Pipit', '023.Brandt_Cormorant', '168.Kentucky_Warbler', '072.Pomarine_Jaeger', '040.Olive_sided_Flycatcher']\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, val test data\n",
    "\n",
    "dir_path = '../Data/images' # data root path\n",
    "image_class_list = os.listdir(dir_path)\n",
    "print(image_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e99fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../Data/images' # data root path\n",
    "count = 0\n",
    "for image_class in image_class_list:\n",
    "    os.makedirs(dir_path +'_train/' + image_class)\n",
    "    os.makedirs(dir_path +'_val/' + image_class)\n",
    "    os.makedirs(dir_path +'_test/' + image_class)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3246c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sources = []\n",
    "\n",
    "all_train_files = []\n",
    "all_val_files = []\n",
    "all_test_files = []\n",
    "\n",
    "all_train_labels = []\n",
    "\n",
    "for i, image_class in enumerate(image_class_list):\n",
    "\n",
    "    source = dir_path + '/' + image_class\n",
    "    all_sources.append(source)\n",
    "    \n",
    "    all_file_names = os.listdir(source)\n",
    "    \n",
    "    np.random.shuffle(all_file_names)\n",
    "    val_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    \n",
    "    train_file_names, val_file_names, test_file_names = np.split(np.array(all_file_names), \\\n",
    "                                                                 [int(len(all_file_names)* (1 - (test_ratio+val_ratio))), \\\n",
    "                                                                  int(len(all_file_names)* (1 - test_ratio))])\n",
    "\n",
    "    train_file_names = [source + '/' + name for name in train_file_names.tolist()]\n",
    "    val_file_names = [source + '/' + name for name in val_file_names.tolist()]\n",
    "    test_file_names = [source + '/' + name for name in test_file_names.tolist()]\n",
    "    \n",
    "    all_train_files.extend(train_file_names)\n",
    "    \n",
    "    all_val_files.extend(val_file_names)\n",
    "    \n",
    "    all_test_files.extend(test_file_names)\n",
    "\n",
    "    # Copy-pasting images\n",
    "    for name in train_file_names:\n",
    "        shutil.copy(name, dir_path +'_train/' + image_class)\n",
    "        all_train_labels.append(i)\n",
    "    for name in val_file_names:\n",
    "        shutil.copy(name, dir_path +'_val/' + image_class)\n",
    "    for name in test_file_names:\n",
    "        shutil.copy(name, dir_path +'_test/' + image_class)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c73e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../Data/images_train'\n",
    "val_path = '../Data/images_val'\n",
    "test_path = '../Data/images_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c14b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 820 files belonging to 20 classes.\n",
      "Found 178 files belonging to 20 classes.\n",
      "Found 178 files belonging to 20 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 06:31:15.713890: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.keras.utils.image_dataset_from_directory(train_path, image_size=(224, 224), label_mode='categorical') \n",
    "val_data = tf.keras.utils.image_dataset_from_directory(val_path, image_size=(224, 224), label_mode='categorical') \n",
    "test_data = tf.keras.utils.image_dataset_from_directory(test_path, image_size=(224, 224), label_mode='categorical') \n",
    "# default batch size: 32\n",
    "# default image size = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "020fcd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457c5a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "501fba51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a12108",
   "metadata": {},
   "source": [
    "#### (c) Transfer Learning\n",
    "i. When dealing with classification of relatively small image datasets, deep networks may not perform very well because of not having enough data to train them. In such cases, one usually uses transfer learning, which uses deep learning models that are trained on very large datasets such as ImageNet as feature extractors. The idea is that such deep networks have learned to extract meaningful features from an image using their layers, and those features can be used in learning other tasks. In order to do that, usually the last layer or the last few layers of the pre-trained network are removed, and the response of the layer before the removed layers to the images in the new dataset is used as a feature vector to train one more multiple replacement layers. The dataset in this task has only around 50-60 images per class. Given that we have 20 classes, training a deep network with such a small dataset may not yield desirable results. In this project, you will use pre-trained models EfficientNetB0 and VGG16. For both pre-trained networks, you will only train the last fully connected layer, and will freeze all layers before them (i.e. we do not change their parameters during training) and use the outputs of the penultimate layer in the original pre-trained model as the features extracted from each image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990980c",
   "metadata": {},
   "source": [
    "ii. To perform empirical regularization, crop, randomly zoo, rotate, flip, contrast, and translate images in your training set for image augmentation. You can use various tools to do this, including OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f9b02",
   "metadata": {},
   "source": [
    "iii. Use ReLU activation functions in the last layer and a softmax layer, along with batch normalization 4 and a dropout rate of 20% as well as ADAM optimizer. Use multinomial cross entropy loss. You can try any batch size, but a batch size of 5 seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cb353",
   "metadata": {},
   "source": [
    "iv. Train the networks (EfficientNetB0 and VGG16) for at least 50 epochs (preferably 100 epochs) and perform early stopping using the validation set. Keep the network parameters that have the lowest validation error. Plot the training and validation errors vs. epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc149947",
   "metadata": {},
   "source": [
    "v. Report Precision, Recall, and F1 score for your model. Remember that this is a multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5d303",
   "metadata": {},
   "source": [
    "## Efficient Net B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ed9641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n",
      "Model: \"efficient_first_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img_augmentation (Sequentia  (None, 224, 224, 3)      0         \n",
      " l)                                                              \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 1000)             5330571   \n",
      "                                                                 \n",
      " output (Dense)              (None, 20)                20020     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,350,591\n",
      "Trainable params: 20,020\n",
      "Non-trainable params: 5,330,571\n",
      "_________________________________________________________________\n",
      "None\n",
      "(None, 20)\n",
      "3.\n",
      "Model: \"efficient_first_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img_augmentation (Sequentia  (None, 224, 224, 3)      0         \n",
      " l)                                                              \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 1000)             5330571   \n",
      "                                                                 \n",
      " output (Dense)              (None, 20)                20020     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 20)               80        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 20)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,350,671\n",
      "Trainable params: 20,060\n",
      "Non-trainable params: 5,330,611\n",
      "_________________________________________________________________\n",
      "None\n",
      "(None, 20)\n"
     ]
    }
   ],
   "source": [
    "img_augmentation = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.RandomCrop(224,224),\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomZoom(height_factor=(-0.1,0.1)),\n",
    "        keras.layers.RandomTranslation(height_factor=(-0.1,0.1), width_factor=(-0.1,0.1)),\n",
    "        keras.layers.RandomRotation(factor=(-0.1, 0.1)),\n",
    "        keras.layers.RandomContrast(factor=0.1)\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")\n",
    "\n",
    "inputs = layers.Input(shape=(224, 224, 3))\n",
    "# x = img_augmentation(inputs)\n",
    "eff_model = EfficientNetB0(weights='imagenet', include_top=True)\n",
    "for layer in eff_model.layers[:-1]:\n",
    "    eff_model.trainable = False \n",
    "eff_model.layers.pop() \n",
    "# keras.utils.plot_model(eff_model, show_shapes=True)\n",
    "\n",
    "# e_model = Sequential([inputs, \\\n",
    "#                       img_augmentation, \\\n",
    "#                       eff_model, \\\n",
    "#                       layers.Dense(20, activation='relu', name='output'), \\\n",
    "#                      ],\n",
    "#                      name='efficient_first_model'\n",
    "#                     )\n",
    "\n",
    "e_model = Sequential(name='efficient_first_model')\n",
    "e_model.add(inputs)\n",
    "e_model.add(img_augmentation)\n",
    "e_model.add(eff_model)\n",
    "e_model.add(layers.Dense(20, activation='relu', name='output'))\n",
    "\n",
    "print('2.')\n",
    "print(e_model.summary())\n",
    "print(e_model.output_shape)\n",
    "\n",
    "e_model.add(Dropout(0.20))\n",
    "e_model.add(tf.keras.layers.BatchNormalization())\n",
    "e_model.add(tf.keras.layers.Softmax())\n",
    "# e_model.build()\n",
    "\n",
    "print('3.')\n",
    "print(e_model.summary())\n",
    "print(e_model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 [==============================] - 64s 2s/step - loss: 2.9799 - accuracy: 0.1280 - val_loss: 2.9876 - val_accuracy: 0.2640\n",
      "Epoch 2/100\n",
      "21/26 [=======================>......] - ETA: 7s - loss: 2.8596 - accuracy: 0.1890"
     ]
    }
   ],
   "source": [
    "# learning_rate=0.1\n",
    "opt = keras.optimizers.Adam()\n",
    "e_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=150, batch_size=15, callbacks=[early_stop])\n",
    "# callbacks = [keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\")]\n",
    "\n",
    "'''\n",
    "print(train_data)\n",
    "print(type(train_data))\n",
    "\n",
    "for img, label in train_data.take(1):\n",
    "    print(img.shape)\n",
    "    print(label.shape)\n",
    "    print('>>>')\n",
    "    print(print(label[0]))\n",
    "    print(label[1])\n",
    "'''\n",
    "# fit the model\n",
    "e_history = e_model.fit(\n",
    "    train_data,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stop],\n",
    "    batch_size=5,\n",
    "    validation_data=val_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"Efficient Net B0 model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(e_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "from sklearn.ensemble._forest import _generate_unsampled_indices\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y = []\n",
    "y_label = []\n",
    "for img, label in test_data:\n",
    "    e_predictions = e_model.predict(img)\n",
    "    # y.append(e_predictions)\n",
    "    for i in range(len(label)):\n",
    "        \n",
    "        cla = np.argmax(e_predictions[i])\n",
    "        class_list = [0] * 20\n",
    "        class_list[cla] = 1\n",
    "        y.append(class_list)\n",
    "\n",
    "        y_label.append(label[i])\n",
    "\n",
    "# y_train_prediction = clf_lr.predict(pruned_features_df)\n",
    "cm = multilabel_confusion_matrix(y_true=y_label, y_pred=y)\n",
    "# print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f077cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "per_class_precision = [0] * 20\n",
    "per_class_recall = [0] * 20\n",
    "per_class_f_score = [0] * 20\n",
    "\n",
    "for idx in range(20):\n",
    "    # print(cm[idx])\n",
    "    tn, fp, fn, tp = cm[idx].ravel()   \n",
    "    precision = (tp) / (tp+fp)\n",
    "    TPR = tp / (tp + fn) \n",
    "    per_class_precision[idx] = precision\n",
    "    per_class_recall[idx] = (tp) / (tp+fn)\n",
    "    \n",
    "    # calculate: F score\n",
    "    f_score = 2 * (precision * TPR) / (precision + TPR)\n",
    "    per_class_f_score[idx] = f_score\n",
    "\n",
    "print('Precision:')\n",
    "for i in range(len(per_class_precision)):\n",
    "    if math.isnan(per_class_precision[i]):\n",
    "        per_class_precision[i] = 'NAN'\n",
    "    print('Class', i+1, ': ', per_class_precision[i])\n",
    "# print(per_class_precision)\n",
    "\n",
    "print()\n",
    "print('Recall:')\n",
    "for i in range(len(per_class_recall)):\n",
    "    print('Class', i+1, ': ', per_class_recall[i])\n",
    "# print(per_class_recall)\n",
    "\n",
    "print()\n",
    "print('F1 score:')\n",
    "for i in range(len(per_class_recall)):\n",
    "    print('Class', i+1, ': ', per_class_recall[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d229de6",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_augmentation = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.RandomCrop(224,224),\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomZoom(height_factor=(-0.1,0.1)),\n",
    "        keras.layers.RandomTranslation(height_factor=(-0.1,0.1), width_factor=(-0.1,0.1)),\n",
    "        keras.layers.RandomRotation(factor=(-0.1, 0.1)),\n",
    "        keras.layers.RandomContrast(factor=0.1)\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")\n",
    "\n",
    "inputs = layers.Input(shape=(224, 224, 3))\n",
    "# x = img_augmentation(inputs)\n",
    "# vgg_model = VGG16(weights=\"imagenet\", include_top=True, input_shape=(224, 224, 3))\n",
    "vgg_model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "for layer in vgg_model.layers[:-1]:\n",
    "    vgg_model.trainable = False \n",
    "vgg_model.layers.pop() \n",
    "# keras.utils.plot_model(vgg_model, show_shapes=True)\n",
    "\n",
    "# e_model = Sequential([inputs, \\\n",
    "#                       img_augmentation, \\\n",
    "#                       eff_model, \\\n",
    "#                       layers.Dense(20, activation='relu', name='output'), \\\n",
    "#                      ],\n",
    "#                      name='efficient_first_model'\n",
    "#                     )\n",
    "\n",
    "v_model = Sequential(name='vgg16_first_model')\n",
    "v_model.add(inputs)\n",
    "v_model.add(img_augmentation)\n",
    "v_model.add(eff_model)\n",
    "v_model.add(layers.Dense(20, activation='relu', name='output'))\n",
    "\n",
    "\n",
    "v_model.add(Dropout(0.20))\n",
    "v_model.add(tf.keras.layers.BatchNormalization())\n",
    "v_model.add(tf.keras.layers.Softmax())\n",
    "# e_model.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5eda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = keras.optimizers.Adam(learning_rate=1)\n",
    "opt = keras.optimizers.Adam()\n",
    "v_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=150, batch_size=15, callbacks=[early_stop])\n",
    "# callbacks = [keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\")]\n",
    "\n",
    "'''\n",
    "print(train_data)\n",
    "print(type(train_data))\n",
    "\n",
    "for img, label in train_data.take(1):\n",
    "    print(img.shape)\n",
    "    print(label.shape)\n",
    "    print('>>>')\n",
    "    print(print(label[0]))\n",
    "    print(label[1])\n",
    "'''\n",
    "# fit the model\n",
    "v_history = v_model.fit(\n",
    "    train_data,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stop],\n",
    "    batch_size=5,\n",
    "    validation_data=val_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3eb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"VGG 16 model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(v_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "y_label = []\n",
    "for img, label in test_data:\n",
    "    v_predictions = v_model.predict(img)\n",
    "    # y.append(e_predictions)\n",
    "    for i in range(len(label)):\n",
    "        \n",
    "        cla = np.argmax(v_predictions[i])\n",
    "        class_list = [0] * 20\n",
    "        class_list[cla] = 1\n",
    "        y.append(class_list)\n",
    "\n",
    "        y_label.append(label[i])\n",
    "\n",
    "# y_train_prediction = clf_lr.predict(pruned_features_df)\n",
    "cm = multilabel_confusion_matrix(y_true=y_label, y_pred=y)\n",
    "# print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79494e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_precision = [0] * 20\n",
    "per_class_recall = [0] * 20\n",
    "per_class_f_score = [0] * 20\n",
    "\n",
    "for idx in range(20):\n",
    "    # print(cm[idx])\n",
    "    tn, fp, fn, tp = cm[idx].ravel()   \n",
    "    precision = (tp) / (tp+fp)\n",
    "    TPR = tp / (tp + fn) \n",
    "    per_class_precision[idx] = precision\n",
    "    per_class_recall[idx] = (tp) / (tp+fn)\n",
    "    \n",
    "    # calculate: F score\n",
    "    f_score = 2 * (precision * TPR) / (precision + TPR)\n",
    "    per_class_f_score[idx] = f_score\n",
    "\n",
    "print('Precision:')\n",
    "for i in range(len(per_class_precision)):\n",
    "    if math.isnan(per_class_precision[i]):\n",
    "        per_class_precision[i] = 'NAN'\n",
    "    print('Class', i+1, ': ', per_class_precision[i])\n",
    "# print(per_class_precision)\n",
    "\n",
    "print()\n",
    "print('Recall:')\n",
    "for i in range(len(per_class_recall)):\n",
    "    print('Class', i+1, ': ', per_class_recall[i])\n",
    "# print(per_class_recall)\n",
    "\n",
    "print()\n",
    "print('F1 score:')\n",
    "for i in range(len(per_class_recall)):\n",
    "    print('Class', i+1, ': ', per_class_recall[i])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
